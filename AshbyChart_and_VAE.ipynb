{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.realpath('./src/'))\n",
    "from materialEncoder import MaterialEncoder\n",
    "\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.patches import Polygon, Ellipse\n",
    "from smallestEllipse import *\n",
    "\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Material         50 non-null     object \n",
      " 1   H (Mohs)/H0      50 non-null     float64\n",
      " 2   E(GPa)/E0        50 non-null     float64\n",
      " 3   UTS (MPa)        50 non-null     float64\n",
      " 4   Rho (g/cc)/Rho0  50 non-null     float64\n",
      " 5   TM (degC)/T0     50 non-null     float64\n",
      " 6   CTE (1/K)/CTE0   50 non-null     float64\n",
      " 7   TC (W/m-K)/TC0   50 non-null     float64\n",
      "dtypes: float64(7), object(1)\n",
      "memory usage: 3.3+ KB\n"
     ]
    }
   ],
   "source": [
    "def check_xl():\n",
    "    df = pd.read_excel('./data/newMaterialData_7D.xlsx')# solidworksMaterialDatabaseCost # aluminum\n",
    "    return df\n",
    "\n",
    "df = check_xl()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indium' 'Lead' 'Tin' 'Titanium' 'Tungsten' 'Molybdenum' 'Zirconium'\n",
      " 'Tantalum' 'Gold' 'Silver' 'Lanthanum' 'Cadmium' 'Zinc' 'Aluminum'\n",
      " 'Antimony' 'Platinum' 'Palladium' 'Nickel' 'Vanadium' 'Iron' 'Copper'\n",
      " 'Chromium' 'Rhodium' 'Magnesium' 'Beryllium' 'Alumina' 'WC'\n",
      " 'Barium Titanate' 'AlN' 'Zirconia' 'MgO' 'Silicon Nitride' 'BeO'\n",
      " 'Boron Carbide' 'Titanium Diboride' 'Boron Nitride' 'Mullite'\n",
      " 'Tantalum Carbide' 'Cordierite' 'a-SiC' 'Fused Silica' 'Titanium Nitride'\n",
      " 'Forsterite' 'TiC' 'Calcium Fluoride' 'Steatite' 'Soda-lime glass'\n",
      " 'Borosilicate glass' 'Brick' 'Concrete']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Youngs Modulus')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df['Rho (g/cc)/Rho0'].values\n",
    "y = df['E(GPa)/E0'].values\n",
    "txt = df['Material'].values\n",
    "arr = np.array([0]*25 + [1]*25)\n",
    "clrs = ['red' if val == 0 else 'blue' for val in arr]\n",
    "\n",
    "\n",
    "print(txt)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, c=clrs)\n",
    "\n",
    "for i, j in enumerate(txt):\n",
    "    ax.annotate(j, (x[i], y[i]))\n",
    "    \n",
    "from smallestEllipse import *\n",
    "\n",
    "for i in [0,1]:\n",
    "    pts = np.array([x[25*i:25*(i+1)], y[25*i:25*(i+1)]]).T\n",
    "    enclosing_ellipse = welzl(np.array(pts, dtype=float))\n",
    "    # plot resulting ellipse\n",
    "    center,a,b,t = enclosing_ellipse\n",
    "    elli = plot_ellipse(enclosing_ellipse, str='k')\n",
    "    ellipse = Ellipse(xy=center, width=2*a, height=2*b, angle=np.degrees(t), edgecolor='k', fc='r' if i == 0 else 'b', alpha=0.3, lw=2)\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "ax.set_xlabel('Density')\n",
    "ax.set_ylabel('Youngs Modulus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2822, 0.0000],\n",
      "        [0.4876, 0.0022],\n",
      "        [0.2817, 0.0515],\n",
      "        [0.1401, 0.1759],\n",
      "        [0.8909, 0.6595],\n",
      "        [0.4303, 0.5403],\n",
      "        [0.2431, 0.1393],\n",
      "        [0.7565, 0.2951],\n",
      "        [0.8919, 0.1098],\n",
      "        [0.4440, 0.1078],\n",
      "        [0.2248, 0.0407],\n",
      "        [0.3501, 0.0724],\n",
      "        [0.2720, 0.1427],\n",
      "        [0.0488, 0.0959],\n",
      "        [0.2476, 0.1108],\n",
      "        [1.0000, 0.2644],\n",
      "        [0.5218, 0.1844],\n",
      "        [0.3637, 0.3189],\n",
      "        [0.2218, 0.1963],\n",
      "        [0.3113, 0.3376],\n",
      "        [0.3664, 0.1997],\n",
      "        [0.2766, 0.4534],\n",
      "        [0.5434, 0.4466],\n",
      "        [0.0000, 0.0550],\n",
      "        [0.0056, 0.4671],\n",
      "        [0.1061, 0.6475],\n",
      "        [0.6982, 1.0000],\n",
      "        [0.2172, 0.0925],\n",
      "        [0.0772, 0.5403],\n",
      "        [0.2172, 0.3189],\n",
      "        [0.0945, 0.4892],\n",
      "        [0.0726, 0.3734],\n",
      "        [0.0645, 0.5658],\n",
      "        [0.0346, 0.6799],\n",
      "        [0.1396, 0.6935],\n",
      "        [0.0184, 0.0805],\n",
      "        [0.0539, 0.2219],\n",
      "        [0.6474, 0.6169],\n",
      "        [0.0437, 0.1912],\n",
      "        [0.0747, 0.7889],\n",
      "        [0.0236, 0.1005],\n",
      "        [0.1766, 0.4058],\n",
      "        [0.0539, 0.2338],\n",
      "        [0.1624, 0.7446],\n",
      "        [0.0732, 0.1074],\n",
      "        [0.0493, 0.2133],\n",
      "        [0.0371, 0.0976],\n",
      "        [0.0260, 0.0848],\n",
      "        [0.0133, 0.0295],\n",
      "        [0.0336, 0.0312]])\n"
     ]
    }
   ],
   "source": [
    "combined_array = np.column_stack((x, y))\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "d = torch.tensor(combined_array, dtype=torch.float32)\n",
    "\n",
    "min_vals = d.min(dim=0, keepdim=True)[0]  # Minimum values for each column\n",
    "max_vals = d.max(dim=0, keepdim=True)[0]  # Maximum values for each column\n",
    "\n",
    "# Apply normalization\n",
    "data = (d - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 73.01908874511719\n",
      "Epoch 2, Loss: 70.69432067871094\n",
      "Epoch 3, Loss: 71.22345733642578\n",
      "Epoch 4, Loss: 69.61856079101562\n",
      "Epoch 5, Loss: 69.81043243408203\n",
      "Epoch 6, Loss: 70.60877227783203\n",
      "Epoch 7, Loss: 69.3958969116211\n",
      "Epoch 8, Loss: 68.55654907226562\n",
      "Epoch 9, Loss: 68.64385986328125\n",
      "Epoch 10, Loss: 69.24551391601562\n",
      "Epoch 11, Loss: 69.59916687011719\n",
      "Epoch 12, Loss: 67.3238754272461\n",
      "Epoch 13, Loss: 66.5539321899414\n",
      "Epoch 14, Loss: 67.81016540527344\n",
      "Epoch 15, Loss: 67.02982330322266\n",
      "Epoch 16, Loss: 66.54633331298828\n",
      "Epoch 17, Loss: 67.61050415039062\n",
      "Epoch 18, Loss: 66.04324340820312\n",
      "Epoch 19, Loss: 66.36332702636719\n",
      "Epoch 20, Loss: 64.8904037475586\n",
      "Epoch 21, Loss: 65.39141845703125\n",
      "Epoch 22, Loss: 64.7639389038086\n",
      "Epoch 23, Loss: 63.08685302734375\n",
      "Epoch 24, Loss: 64.98759460449219\n",
      "Epoch 25, Loss: 65.69403076171875\n",
      "Epoch 26, Loss: 63.41567611694336\n",
      "Epoch 27, Loss: 63.069122314453125\n",
      "Epoch 28, Loss: 63.50511169433594\n",
      "Epoch 29, Loss: 63.666988372802734\n",
      "Epoch 30, Loss: 62.81449508666992\n",
      "Epoch 31, Loss: 63.044898986816406\n",
      "Epoch 32, Loss: 62.64137649536133\n",
      "Epoch 33, Loss: 62.635353088378906\n",
      "Epoch 34, Loss: 62.7520751953125\n",
      "Epoch 35, Loss: 61.706146240234375\n",
      "Epoch 36, Loss: 63.6005744934082\n",
      "Epoch 37, Loss: 60.243892669677734\n",
      "Epoch 38, Loss: 62.393123626708984\n",
      "Epoch 39, Loss: 60.40693283081055\n",
      "Epoch 40, Loss: 61.94296646118164\n",
      "Epoch 41, Loss: 61.47650909423828\n",
      "Epoch 42, Loss: 61.10681915283203\n",
      "Epoch 43, Loss: 61.50128173828125\n",
      "Epoch 44, Loss: 60.84199142456055\n",
      "Epoch 45, Loss: 61.122379302978516\n",
      "Epoch 46, Loss: 60.950592041015625\n",
      "Epoch 47, Loss: 61.516441345214844\n",
      "Epoch 48, Loss: 61.57814407348633\n",
      "Epoch 49, Loss: 60.46200942993164\n",
      "Epoch 50, Loss: 60.96427917480469\n",
      "Epoch 51, Loss: 59.688785552978516\n",
      "Epoch 52, Loss: 60.76046371459961\n",
      "Epoch 53, Loss: 59.105655670166016\n",
      "Epoch 54, Loss: 60.25191116333008\n",
      "Epoch 55, Loss: 59.38227081298828\n",
      "Epoch 56, Loss: 59.65814208984375\n",
      "Epoch 57, Loss: 61.12992477416992\n",
      "Epoch 58, Loss: 60.453060150146484\n",
      "Epoch 59, Loss: 60.95045471191406\n",
      "Epoch 60, Loss: 60.212093353271484\n",
      "Epoch 61, Loss: 61.6577262878418\n",
      "Epoch 62, Loss: 60.17709732055664\n",
      "Epoch 63, Loss: 58.857784271240234\n",
      "Epoch 64, Loss: 58.69294738769531\n",
      "Epoch 65, Loss: 59.6299934387207\n",
      "Epoch 66, Loss: 62.18696594238281\n",
      "Epoch 67, Loss: 58.994651794433594\n",
      "Epoch 68, Loss: 58.84048080444336\n",
      "Epoch 69, Loss: 59.566383361816406\n",
      "Epoch 70, Loss: 60.27162551879883\n",
      "Epoch 71, Loss: 60.32101058959961\n",
      "Epoch 72, Loss: 59.40678787231445\n",
      "Epoch 73, Loss: 59.75175476074219\n",
      "Epoch 74, Loss: 58.980709075927734\n",
      "Epoch 75, Loss: 59.316993713378906\n",
      "Epoch 76, Loss: 57.8349723815918\n",
      "Epoch 77, Loss: 59.76068878173828\n",
      "Epoch 78, Loss: 58.32111740112305\n",
      "Epoch 79, Loss: 58.435447692871094\n",
      "Epoch 80, Loss: 59.459251403808594\n",
      "Epoch 81, Loss: 57.7591438293457\n",
      "Epoch 82, Loss: 59.81211853027344\n",
      "Epoch 83, Loss: 59.83477020263672\n",
      "Epoch 84, Loss: 59.79467010498047\n",
      "Epoch 85, Loss: 59.69451141357422\n",
      "Epoch 86, Loss: 59.420692443847656\n",
      "Epoch 87, Loss: 58.97636413574219\n",
      "Epoch 88, Loss: 59.16738510131836\n",
      "Epoch 89, Loss: 58.337650299072266\n",
      "Epoch 90, Loss: 58.18352127075195\n",
      "Epoch 91, Loss: 59.08406448364258\n",
      "Epoch 92, Loss: 59.75802230834961\n",
      "Epoch 93, Loss: 59.33403015136719\n",
      "Epoch 94, Loss: 58.54366683959961\n",
      "Epoch 95, Loss: 58.26499938964844\n",
      "Epoch 96, Loss: 59.17919921875\n",
      "Epoch 97, Loss: 59.12557601928711\n",
      "Epoch 98, Loss: 59.11151123046875\n",
      "Epoch 99, Loss: 59.24005889892578\n",
      "Epoch 100, Loss: 57.97212600708008\n",
      "Epoch 101, Loss: 58.908695220947266\n",
      "Epoch 102, Loss: 56.902164459228516\n",
      "Epoch 103, Loss: 58.45097732543945\n",
      "Epoch 104, Loss: 58.44243240356445\n",
      "Epoch 105, Loss: 57.17737579345703\n",
      "Epoch 106, Loss: 59.149742126464844\n",
      "Epoch 107, Loss: 57.90607833862305\n",
      "Epoch 108, Loss: 59.85558319091797\n",
      "Epoch 109, Loss: 58.304222106933594\n",
      "Epoch 110, Loss: 58.28836441040039\n",
      "Epoch 111, Loss: 58.56306838989258\n",
      "Epoch 112, Loss: 58.13670349121094\n",
      "Epoch 113, Loss: 58.30427169799805\n",
      "Epoch 114, Loss: 58.1019287109375\n",
      "Epoch 115, Loss: 58.51844024658203\n",
      "Epoch 116, Loss: 57.95679473876953\n",
      "Epoch 117, Loss: 58.04273223876953\n",
      "Epoch 118, Loss: 57.55350112915039\n",
      "Epoch 119, Loss: 56.69423294067383\n",
      "Epoch 120, Loss: 58.30983352661133\n",
      "Epoch 121, Loss: 57.46501159667969\n",
      "Epoch 122, Loss: 58.482147216796875\n",
      "Epoch 123, Loss: 58.636173248291016\n",
      "Epoch 124, Loss: 58.3917350769043\n",
      "Epoch 125, Loss: 57.57637405395508\n",
      "Epoch 126, Loss: 57.089962005615234\n",
      "Epoch 127, Loss: 57.15932083129883\n",
      "Epoch 128, Loss: 57.70957565307617\n",
      "Epoch 129, Loss: 57.144927978515625\n",
      "Epoch 130, Loss: 57.545719146728516\n",
      "Epoch 131, Loss: 57.75000762939453\n",
      "Epoch 132, Loss: 56.92135238647461\n",
      "Epoch 133, Loss: 58.015380859375\n",
      "Epoch 134, Loss: 57.54232406616211\n",
      "Epoch 135, Loss: 57.634029388427734\n",
      "Epoch 136, Loss: 57.05324935913086\n",
      "Epoch 137, Loss: 57.88846969604492\n",
      "Epoch 138, Loss: 56.97834014892578\n",
      "Epoch 139, Loss: 57.03950119018555\n",
      "Epoch 140, Loss: 56.68863296508789\n",
      "Epoch 141, Loss: 57.50395584106445\n",
      "Epoch 142, Loss: 57.342185974121094\n",
      "Epoch 143, Loss: 56.46790313720703\n",
      "Epoch 144, Loss: 56.70454406738281\n",
      "Epoch 145, Loss: 56.811058044433594\n",
      "Epoch 146, Loss: 56.58533477783203\n",
      "Epoch 147, Loss: 57.413204193115234\n",
      "Epoch 148, Loss: 56.6804313659668\n",
      "Epoch 149, Loss: 56.759185791015625\n",
      "Epoch 150, Loss: 56.87836837768555\n",
      "Epoch 151, Loss: 56.79723358154297\n",
      "Epoch 152, Loss: 56.84370422363281\n",
      "Epoch 153, Loss: 56.50291442871094\n",
      "Epoch 154, Loss: 56.76750183105469\n",
      "Epoch 155, Loss: 56.4059944152832\n",
      "Epoch 156, Loss: 56.34072494506836\n",
      "Epoch 157, Loss: 55.37615966796875\n",
      "Epoch 158, Loss: 55.743736267089844\n",
      "Epoch 159, Loss: 55.74249267578125\n",
      "Epoch 160, Loss: 56.19446563720703\n",
      "Epoch 161, Loss: 55.98106002807617\n",
      "Epoch 162, Loss: 56.05852127075195\n",
      "Epoch 163, Loss: 56.1506233215332\n",
      "Epoch 164, Loss: 55.76829528808594\n",
      "Epoch 165, Loss: 56.135162353515625\n",
      "Epoch 166, Loss: 55.764915466308594\n",
      "Epoch 167, Loss: 55.92021942138672\n",
      "Epoch 168, Loss: 56.181575775146484\n",
      "Epoch 169, Loss: 56.24933624267578\n",
      "Epoch 170, Loss: 55.759185791015625\n",
      "Epoch 171, Loss: 55.81968307495117\n",
      "Epoch 172, Loss: 55.850303649902344\n",
      "Epoch 173, Loss: 56.11263656616211\n",
      "Epoch 174, Loss: 56.678768157958984\n",
      "Epoch 175, Loss: 55.451316833496094\n",
      "Epoch 176, Loss: 55.41334915161133\n",
      "Epoch 177, Loss: 56.1188850402832\n",
      "Epoch 178, Loss: 55.53983688354492\n",
      "Epoch 179, Loss: 56.37363815307617\n",
      "Epoch 180, Loss: 55.500030517578125\n",
      "Epoch 181, Loss: 55.12321472167969\n",
      "Epoch 182, Loss: 54.86674880981445\n",
      "Epoch 183, Loss: 55.061893463134766\n",
      "Epoch 184, Loss: 54.82611083984375\n",
      "Epoch 185, Loss: 55.10365295410156\n",
      "Epoch 186, Loss: 54.94002151489258\n",
      "Epoch 187, Loss: 54.73124313354492\n",
      "Epoch 188, Loss: 54.87070083618164\n",
      "Epoch 189, Loss: 55.024383544921875\n",
      "Epoch 190, Loss: 54.235145568847656\n",
      "Epoch 191, Loss: 53.493873596191406\n",
      "Epoch 192, Loss: 54.809783935546875\n",
      "Epoch 193, Loss: 54.69771957397461\n",
      "Epoch 194, Loss: 54.48413848876953\n",
      "Epoch 195, Loss: 54.05624771118164\n",
      "Epoch 196, Loss: 54.501434326171875\n",
      "Epoch 197, Loss: 54.7087516784668\n",
      "Epoch 198, Loss: 54.47673034667969\n",
      "Epoch 199, Loss: 54.19327926635742\n",
      "Epoch 200, Loss: 54.393035888671875\n",
      "Epoch 201, Loss: 54.45201873779297\n",
      "Epoch 202, Loss: 54.23398208618164\n",
      "Epoch 203, Loss: 53.69379425048828\n",
      "Epoch 204, Loss: 53.68458557128906\n",
      "Epoch 205, Loss: 53.275142669677734\n",
      "Epoch 206, Loss: 53.98423767089844\n",
      "Epoch 207, Loss: 54.1801643371582\n",
      "Epoch 208, Loss: 53.95579528808594\n",
      "Epoch 209, Loss: 54.29777526855469\n",
      "Epoch 210, Loss: 53.06925582885742\n",
      "Epoch 211, Loss: 53.08512496948242\n",
      "Epoch 212, Loss: 53.71604537963867\n",
      "Epoch 213, Loss: 53.1439094543457\n",
      "Epoch 214, Loss: 52.669307708740234\n",
      "Epoch 215, Loss: 52.871742248535156\n",
      "Epoch 216, Loss: 53.029273986816406\n",
      "Epoch 217, Loss: 52.57626724243164\n",
      "Epoch 218, Loss: 53.07107925415039\n",
      "Epoch 219, Loss: 52.8026237487793\n",
      "Epoch 220, Loss: 52.76115036010742\n",
      "Epoch 221, Loss: 52.154109954833984\n",
      "Epoch 222, Loss: 52.56565475463867\n",
      "Epoch 223, Loss: 52.6815071105957\n",
      "Epoch 224, Loss: 53.1442756652832\n",
      "Epoch 225, Loss: 52.586265563964844\n",
      "Epoch 226, Loss: 52.23786163330078\n",
      "Epoch 227, Loss: 52.040000915527344\n",
      "Epoch 228, Loss: 51.847801208496094\n",
      "Epoch 229, Loss: 52.22248077392578\n",
      "Epoch 230, Loss: 52.16915512084961\n",
      "Epoch 231, Loss: 51.87858200073242\n",
      "Epoch 232, Loss: 52.54917526245117\n",
      "Epoch 233, Loss: 52.3490104675293\n",
      "Epoch 234, Loss: 51.60235595703125\n",
      "Epoch 235, Loss: 51.984806060791016\n",
      "Epoch 236, Loss: 51.7576789855957\n",
      "Epoch 237, Loss: 52.36569595336914\n",
      "Epoch 238, Loss: 51.9680061340332\n",
      "Epoch 239, Loss: 52.45380401611328\n",
      "Epoch 240, Loss: 51.19179153442383\n",
      "Epoch 241, Loss: 51.3364372253418\n",
      "Epoch 242, Loss: 51.28264617919922\n",
      "Epoch 243, Loss: 50.910037994384766\n",
      "Epoch 244, Loss: 51.760948181152344\n",
      "Epoch 245, Loss: 51.14503860473633\n",
      "Epoch 246, Loss: 51.28166198730469\n",
      "Epoch 247, Loss: 50.9208869934082\n",
      "Epoch 248, Loss: 51.23375701904297\n",
      "Epoch 249, Loss: 51.60972213745117\n",
      "Epoch 250, Loss: 50.91923522949219\n",
      "Epoch 251, Loss: 51.02998733520508\n",
      "Epoch 252, Loss: 51.20823287963867\n",
      "Epoch 253, Loss: 50.72993087768555\n",
      "Epoch 254, Loss: 50.39360809326172\n",
      "Epoch 255, Loss: 50.93356704711914\n",
      "Epoch 256, Loss: 51.506996154785156\n",
      "Epoch 257, Loss: 50.240726470947266\n",
      "Epoch 258, Loss: 50.4473876953125\n",
      "Epoch 259, Loss: 50.69499588012695\n",
      "Epoch 260, Loss: 50.787689208984375\n",
      "Epoch 261, Loss: 50.75482177734375\n",
      "Epoch 262, Loss: 50.580623626708984\n",
      "Epoch 263, Loss: 50.35385513305664\n",
      "Epoch 264, Loss: 50.31997299194336\n",
      "Epoch 265, Loss: 50.62968826293945\n",
      "Epoch 266, Loss: 50.25755310058594\n",
      "Epoch 267, Loss: 50.26007080078125\n",
      "Epoch 268, Loss: 49.93903732299805\n",
      "Epoch 269, Loss: 50.42801284790039\n",
      "Epoch 270, Loss: 49.80525207519531\n",
      "Epoch 271, Loss: 50.845394134521484\n",
      "Epoch 272, Loss: 50.55097198486328\n",
      "Epoch 273, Loss: 49.77085494995117\n",
      "Epoch 274, Loss: 50.293853759765625\n",
      "Epoch 275, Loss: 50.394569396972656\n",
      "Epoch 276, Loss: 49.763763427734375\n",
      "Epoch 277, Loss: 50.24715042114258\n",
      "Epoch 278, Loss: 50.215145111083984\n",
      "Epoch 279, Loss: 49.89149475097656\n",
      "Epoch 280, Loss: 49.825260162353516\n",
      "Epoch 281, Loss: 49.7811393737793\n",
      "Epoch 282, Loss: 49.585391998291016\n",
      "Epoch 283, Loss: 49.611473083496094\n",
      "Epoch 284, Loss: 49.61728286743164\n",
      "Epoch 285, Loss: 49.28690719604492\n",
      "Epoch 286, Loss: 49.42778015136719\n",
      "Epoch 287, Loss: 49.72602081298828\n",
      "Epoch 288, Loss: 49.37330627441406\n",
      "Epoch 289, Loss: 49.54847717285156\n",
      "Epoch 290, Loss: 48.87644958496094\n",
      "Epoch 291, Loss: 49.52009201049805\n",
      "Epoch 292, Loss: 48.867027282714844\n",
      "Epoch 293, Loss: 49.1455078125\n",
      "Epoch 294, Loss: 49.192657470703125\n",
      "Epoch 295, Loss: 49.03706741333008\n",
      "Epoch 296, Loss: 49.13626480102539\n",
      "Epoch 297, Loss: 49.08328628540039\n",
      "Epoch 298, Loss: 49.00065612792969\n",
      "Epoch 299, Loss: 49.12095260620117\n",
      "Epoch 300, Loss: 48.99656677246094\n",
      "Epoch 301, Loss: 48.818756103515625\n",
      "Epoch 302, Loss: 48.615726470947266\n",
      "Epoch 303, Loss: 48.547054290771484\n",
      "Epoch 304, Loss: 48.80269241333008\n",
      "Epoch 305, Loss: 48.753177642822266\n",
      "Epoch 306, Loss: 48.157188415527344\n",
      "Epoch 307, Loss: 48.383567810058594\n",
      "Epoch 308, Loss: 48.9549674987793\n",
      "Epoch 309, Loss: 48.33181381225586\n",
      "Epoch 310, Loss: 48.40510940551758\n",
      "Epoch 311, Loss: 48.32650375366211\n",
      "Epoch 312, Loss: 48.2358512878418\n",
      "Epoch 313, Loss: 48.14332580566406\n",
      "Epoch 314, Loss: 48.983863830566406\n",
      "Epoch 315, Loss: 48.02202606201172\n",
      "Epoch 316, Loss: 48.03704833984375\n",
      "Epoch 317, Loss: 47.929439544677734\n",
      "Epoch 318, Loss: 47.753562927246094\n",
      "Epoch 319, Loss: 47.83331298828125\n",
      "Epoch 320, Loss: 47.48505783081055\n",
      "Epoch 321, Loss: 47.786415100097656\n",
      "Epoch 322, Loss: 47.86235046386719\n",
      "Epoch 323, Loss: 47.84951400756836\n",
      "Epoch 324, Loss: 47.38225555419922\n",
      "Epoch 325, Loss: 47.2580451965332\n",
      "Epoch 326, Loss: 47.3936882019043\n",
      "Epoch 327, Loss: 47.202125549316406\n",
      "Epoch 328, Loss: 47.191688537597656\n",
      "Epoch 329, Loss: 47.97224426269531\n",
      "Epoch 330, Loss: 47.107418060302734\n",
      "Epoch 331, Loss: 46.68622589111328\n",
      "Epoch 332, Loss: 46.766326904296875\n",
      "Epoch 333, Loss: 46.821533203125\n",
      "Epoch 334, Loss: 46.4567756652832\n",
      "Epoch 335, Loss: 46.827110290527344\n",
      "Epoch 336, Loss: 46.825096130371094\n",
      "Epoch 337, Loss: 46.83686447143555\n",
      "Epoch 338, Loss: 46.76596450805664\n",
      "Epoch 339, Loss: 46.72283935546875\n",
      "Epoch 340, Loss: 46.77790069580078\n",
      "Epoch 341, Loss: 46.193702697753906\n",
      "Epoch 342, Loss: 46.45051574707031\n",
      "Epoch 343, Loss: 46.11194610595703\n",
      "Epoch 344, Loss: 46.00851821899414\n",
      "Epoch 345, Loss: 45.8803596496582\n",
      "Epoch 346, Loss: 46.49367141723633\n",
      "Epoch 347, Loss: 45.993003845214844\n",
      "Epoch 348, Loss: 45.69139862060547\n",
      "Epoch 349, Loss: 45.447914123535156\n",
      "Epoch 350, Loss: 46.046939849853516\n",
      "Epoch 351, Loss: 45.85171127319336\n",
      "Epoch 352, Loss: 45.948890686035156\n",
      "Epoch 353, Loss: 45.731712341308594\n",
      "Epoch 354, Loss: 45.48361587524414\n",
      "Epoch 355, Loss: 45.381431579589844\n",
      "Epoch 356, Loss: 45.44695281982422\n",
      "Epoch 357, Loss: 45.24372863769531\n",
      "Epoch 358, Loss: 45.75442123413086\n",
      "Epoch 359, Loss: 45.301124572753906\n",
      "Epoch 360, Loss: 45.45351028442383\n",
      "Epoch 361, Loss: 45.03794860839844\n",
      "Epoch 362, Loss: 45.7171516418457\n",
      "Epoch 363, Loss: 45.26350402832031\n",
      "Epoch 364, Loss: 44.80835723876953\n",
      "Epoch 365, Loss: 44.89251708984375\n",
      "Epoch 366, Loss: 44.963340759277344\n",
      "Epoch 367, Loss: 44.97146987915039\n",
      "Epoch 368, Loss: 44.80833053588867\n",
      "Epoch 369, Loss: 44.81296920776367\n",
      "Epoch 370, Loss: 44.655513763427734\n",
      "Epoch 371, Loss: 44.864654541015625\n",
      "Epoch 372, Loss: 44.79267883300781\n",
      "Epoch 373, Loss: 44.54442596435547\n",
      "Epoch 374, Loss: 44.70802307128906\n",
      "Epoch 375, Loss: 44.525264739990234\n",
      "Epoch 376, Loss: 44.46025085449219\n",
      "Epoch 377, Loss: 44.46241760253906\n",
      "Epoch 378, Loss: 44.442832946777344\n",
      "Epoch 379, Loss: 44.275875091552734\n",
      "Epoch 380, Loss: 44.581905364990234\n",
      "Epoch 381, Loss: 44.29084014892578\n",
      "Epoch 382, Loss: 44.19480514526367\n",
      "Epoch 383, Loss: 44.429840087890625\n",
      "Epoch 384, Loss: 44.239173889160156\n",
      "Epoch 385, Loss: 44.16838455200195\n",
      "Epoch 386, Loss: 44.21882629394531\n",
      "Epoch 387, Loss: 44.1543083190918\n",
      "Epoch 388, Loss: 43.999263763427734\n",
      "Epoch 389, Loss: 44.01986312866211\n",
      "Epoch 390, Loss: 43.972415924072266\n",
      "Epoch 391, Loss: 43.964290618896484\n",
      "Epoch 392, Loss: 44.17560958862305\n",
      "Epoch 393, Loss: 44.11204528808594\n",
      "Epoch 394, Loss: 43.76947021484375\n",
      "Epoch 395, Loss: 44.02606201171875\n",
      "Epoch 396, Loss: 43.95823669433594\n",
      "Epoch 397, Loss: 44.100440979003906\n",
      "Epoch 398, Loss: 43.73849868774414\n",
      "Epoch 399, Loss: 44.017250061035156\n",
      "Epoch 400, Loss: 43.95963668823242\n",
      "Epoch 401, Loss: 43.962100982666016\n",
      "Epoch 402, Loss: 43.8062744140625\n",
      "Epoch 403, Loss: 43.96127700805664\n",
      "Epoch 404, Loss: 43.704959869384766\n",
      "Epoch 405, Loss: 43.637760162353516\n",
      "Epoch 406, Loss: 43.87588119506836\n",
      "Epoch 407, Loss: 43.655399322509766\n",
      "Epoch 408, Loss: 43.754608154296875\n",
      "Epoch 409, Loss: 43.8157958984375\n",
      "Epoch 410, Loss: 43.55147933959961\n",
      "Epoch 411, Loss: 43.61809539794922\n",
      "Epoch 412, Loss: 43.452545166015625\n",
      "Epoch 413, Loss: 43.52459716796875\n",
      "Epoch 414, Loss: 43.57250213623047\n",
      "Epoch 415, Loss: 43.47520065307617\n",
      "Epoch 416, Loss: 43.62430953979492\n",
      "Epoch 417, Loss: 43.50887680053711\n",
      "Epoch 418, Loss: 43.454715728759766\n",
      "Epoch 419, Loss: 43.659385681152344\n",
      "Epoch 420, Loss: 43.468788146972656\n",
      "Epoch 421, Loss: 43.47206115722656\n",
      "Epoch 422, Loss: 43.66358184814453\n",
      "Epoch 423, Loss: 43.662681579589844\n",
      "Epoch 424, Loss: 43.712459564208984\n",
      "Epoch 425, Loss: 43.54581832885742\n",
      "Epoch 426, Loss: 43.49297332763672\n",
      "Epoch 427, Loss: 43.64663314819336\n",
      "Epoch 428, Loss: 43.3857536315918\n",
      "Epoch 429, Loss: 43.48391342163086\n",
      "Epoch 430, Loss: 43.654747009277344\n",
      "Epoch 431, Loss: 43.496665954589844\n",
      "Epoch 432, Loss: 43.40892028808594\n",
      "Epoch 433, Loss: 43.339290618896484\n",
      "Epoch 434, Loss: 43.36284637451172\n",
      "Epoch 435, Loss: 43.57448196411133\n",
      "Epoch 436, Loss: 43.46937561035156\n",
      "Epoch 437, Loss: 43.55495834350586\n",
      "Epoch 438, Loss: 43.47718811035156\n",
      "Epoch 439, Loss: 43.29652404785156\n",
      "Epoch 440, Loss: 43.471065521240234\n",
      "Epoch 441, Loss: 43.29829025268555\n",
      "Epoch 442, Loss: 43.28886413574219\n",
      "Epoch 443, Loss: 43.4297981262207\n",
      "Epoch 444, Loss: 43.34743118286133\n",
      "Epoch 445, Loss: 43.349952697753906\n",
      "Epoch 446, Loss: 43.38102722167969\n",
      "Epoch 447, Loss: 43.26222229003906\n",
      "Epoch 448, Loss: 43.27402114868164\n",
      "Epoch 449, Loss: 43.28548049926758\n",
      "Epoch 450, Loss: 43.38890075683594\n",
      "Epoch 451, Loss: 43.468528747558594\n",
      "Epoch 452, Loss: 43.36556625366211\n",
      "Epoch 453, Loss: 43.227256774902344\n",
      "Epoch 454, Loss: 43.381996154785156\n",
      "Epoch 455, Loss: 43.33967590332031\n",
      "Epoch 456, Loss: 43.34346008300781\n",
      "Epoch 457, Loss: 43.48313522338867\n",
      "Epoch 458, Loss: 43.40291976928711\n",
      "Epoch 459, Loss: 43.28639602661133\n",
      "Epoch 460, Loss: 43.26728820800781\n",
      "Epoch 461, Loss: 43.315670013427734\n",
      "Epoch 462, Loss: 43.32795333862305\n",
      "Epoch 463, Loss: 43.33503723144531\n",
      "Epoch 464, Loss: 43.2204704284668\n",
      "Epoch 465, Loss: 43.54591751098633\n",
      "Epoch 466, Loss: 43.312217712402344\n",
      "Epoch 467, Loss: 43.300445556640625\n",
      "Epoch 468, Loss: 43.24140930175781\n",
      "Epoch 469, Loss: 43.36797332763672\n",
      "Epoch 470, Loss: 43.269615173339844\n",
      "Epoch 471, Loss: 43.15028381347656\n",
      "Epoch 472, Loss: 43.34672546386719\n",
      "Epoch 473, Loss: 43.280696868896484\n",
      "Epoch 474, Loss: 43.29050064086914\n",
      "Epoch 475, Loss: 43.267154693603516\n",
      "Epoch 476, Loss: 43.2191162109375\n",
      "Epoch 477, Loss: 43.3054313659668\n",
      "Epoch 478, Loss: 43.22829055786133\n",
      "Epoch 479, Loss: 43.10743713378906\n",
      "Epoch 480, Loss: 43.14943313598633\n",
      "Epoch 481, Loss: 43.21906661987305\n",
      "Epoch 482, Loss: 43.260772705078125\n",
      "Epoch 483, Loss: 43.173545837402344\n",
      "Epoch 484, Loss: 43.215232849121094\n",
      "Epoch 485, Loss: 43.17700958251953\n",
      "Epoch 486, Loss: 43.153533935546875\n",
      "Epoch 487, Loss: 43.11643600463867\n",
      "Epoch 488, Loss: 43.188865661621094\n",
      "Epoch 489, Loss: 43.18244552612305\n",
      "Epoch 490, Loss: 43.24802017211914\n",
      "Epoch 491, Loss: 43.15761184692383\n",
      "Epoch 492, Loss: 43.19169616699219\n",
      "Epoch 493, Loss: 43.222896575927734\n",
      "Epoch 494, Loss: 43.22932815551758\n",
      "Epoch 495, Loss: 43.10139846801758\n",
      "Epoch 496, Loss: 43.12431716918945\n",
      "Epoch 497, Loss: 43.16547393798828\n",
      "Epoch 498, Loss: 43.25387954711914\n",
      "Epoch 499, Loss: 43.132774353027344\n",
      "Epoch 500, Loss: 43.22207260131836\n",
      "Epoch 501, Loss: 43.17873001098633\n",
      "Epoch 502, Loss: 43.09709167480469\n",
      "Epoch 503, Loss: 43.09994888305664\n",
      "Epoch 504, Loss: 43.19816207885742\n",
      "Epoch 505, Loss: 43.154502868652344\n",
      "Epoch 506, Loss: 43.208587646484375\n",
      "Epoch 507, Loss: 43.078285217285156\n",
      "Epoch 508, Loss: 43.06352233886719\n",
      "Epoch 509, Loss: 43.10633087158203\n",
      "Epoch 510, Loss: 43.13336181640625\n",
      "Epoch 511, Loss: 43.12290573120117\n",
      "Epoch 512, Loss: 43.123111724853516\n",
      "Epoch 513, Loss: 43.1063117980957\n",
      "Epoch 514, Loss: 43.12409210205078\n",
      "Epoch 515, Loss: 43.06504440307617\n",
      "Epoch 516, Loss: 43.14875793457031\n",
      "Epoch 517, Loss: 43.129085540771484\n",
      "Epoch 518, Loss: 43.05732345581055\n",
      "Epoch 519, Loss: 43.08592224121094\n",
      "Epoch 520, Loss: 43.122467041015625\n",
      "Epoch 521, Loss: 43.06987380981445\n",
      "Epoch 522, Loss: 43.0944709777832\n",
      "Epoch 523, Loss: 43.11775207519531\n",
      "Epoch 524, Loss: 43.09328842163086\n",
      "Epoch 525, Loss: 43.108612060546875\n",
      "Epoch 526, Loss: 43.078983306884766\n",
      "Epoch 527, Loss: 43.065879821777344\n",
      "Epoch 528, Loss: 43.07546615600586\n",
      "Epoch 529, Loss: 43.06499099731445\n",
      "Epoch 530, Loss: 43.0777473449707\n",
      "Epoch 531, Loss: 43.08008575439453\n",
      "Epoch 532, Loss: 43.05812454223633\n",
      "Epoch 533, Loss: 42.99369430541992\n",
      "Epoch 534, Loss: 43.128204345703125\n",
      "Epoch 535, Loss: 43.0315055847168\n",
      "Epoch 536, Loss: 43.11489486694336\n",
      "Epoch 537, Loss: 43.108219146728516\n",
      "Epoch 538, Loss: 43.082218170166016\n",
      "Epoch 539, Loss: 43.09217834472656\n",
      "Epoch 540, Loss: 43.13920974731445\n",
      "Epoch 541, Loss: 43.09877014160156\n",
      "Epoch 542, Loss: 43.01593780517578\n",
      "Epoch 543, Loss: 43.050235748291016\n",
      "Epoch 544, Loss: 43.074581146240234\n",
      "Epoch 545, Loss: 43.092308044433594\n",
      "Epoch 546, Loss: 43.08640670776367\n",
      "Epoch 547, Loss: 43.04085159301758\n",
      "Epoch 548, Loss: 43.040199279785156\n",
      "Epoch 549, Loss: 43.0286979675293\n",
      "Epoch 550, Loss: 43.066558837890625\n",
      "Epoch 551, Loss: 43.04380798339844\n",
      "Epoch 552, Loss: 43.033203125\n",
      "Epoch 553, Loss: 43.089500427246094\n",
      "Epoch 554, Loss: 43.00468444824219\n",
      "Epoch 555, Loss: 43.03306579589844\n",
      "Epoch 556, Loss: 43.09510040283203\n",
      "Epoch 557, Loss: 43.0212287902832\n",
      "Epoch 558, Loss: 43.03396987915039\n",
      "Epoch 559, Loss: 43.06017303466797\n",
      "Epoch 560, Loss: 43.03285217285156\n",
      "Epoch 561, Loss: 42.982330322265625\n",
      "Epoch 562, Loss: 43.00119400024414\n",
      "Epoch 563, Loss: 43.06945037841797\n",
      "Epoch 564, Loss: 43.0023078918457\n",
      "Epoch 565, Loss: 43.01387023925781\n",
      "Epoch 566, Loss: 43.07076644897461\n",
      "Epoch 567, Loss: 43.04771041870117\n",
      "Epoch 568, Loss: 43.023860931396484\n",
      "Epoch 569, Loss: 43.02637481689453\n",
      "Epoch 570, Loss: 43.00167465209961\n",
      "Epoch 571, Loss: 43.009124755859375\n",
      "Epoch 572, Loss: 43.05345916748047\n",
      "Epoch 573, Loss: 43.03403091430664\n",
      "Epoch 574, Loss: 43.05051803588867\n",
      "Epoch 575, Loss: 42.98617172241211\n",
      "Epoch 576, Loss: 43.02815246582031\n",
      "Epoch 577, Loss: 43.01240539550781\n",
      "Epoch 578, Loss: 43.033782958984375\n",
      "Epoch 579, Loss: 43.057621002197266\n",
      "Epoch 580, Loss: 43.0258674621582\n",
      "Epoch 581, Loss: 43.02142333984375\n",
      "Epoch 582, Loss: 42.994529724121094\n",
      "Epoch 583, Loss: 43.03245162963867\n",
      "Epoch 584, Loss: 42.96780776977539\n",
      "Epoch 585, Loss: 42.984535217285156\n",
      "Epoch 586, Loss: 42.99403762817383\n",
      "Epoch 587, Loss: 43.0401496887207\n",
      "Epoch 588, Loss: 43.06852340698242\n",
      "Epoch 589, Loss: 42.966304779052734\n",
      "Epoch 590, Loss: 42.980133056640625\n",
      "Epoch 591, Loss: 43.00109100341797\n",
      "Epoch 592, Loss: 42.95065689086914\n",
      "Epoch 593, Loss: 42.946964263916016\n",
      "Epoch 594, Loss: 42.967864990234375\n",
      "Epoch 595, Loss: 43.01891326904297\n",
      "Epoch 596, Loss: 42.93745803833008\n",
      "Epoch 597, Loss: 42.999359130859375\n",
      "Epoch 598, Loss: 42.97354507446289\n",
      "Epoch 599, Loss: 42.983238220214844\n",
      "Epoch 600, Loss: 42.960323333740234\n",
      "Epoch 601, Loss: 42.98385238647461\n",
      "Epoch 602, Loss: 42.94794464111328\n",
      "Epoch 603, Loss: 42.98817443847656\n",
      "Epoch 604, Loss: 43.0024299621582\n",
      "Epoch 605, Loss: 42.934085845947266\n",
      "Epoch 606, Loss: 43.00824737548828\n",
      "Epoch 607, Loss: 42.99298858642578\n",
      "Epoch 608, Loss: 42.9476432800293\n",
      "Epoch 609, Loss: 42.99820327758789\n",
      "Epoch 610, Loss: 42.94673156738281\n",
      "Epoch 611, Loss: 42.965274810791016\n",
      "Epoch 612, Loss: 43.0185546875\n",
      "Epoch 613, Loss: 42.956634521484375\n",
      "Epoch 614, Loss: 42.938011169433594\n",
      "Epoch 615, Loss: 42.9133415222168\n",
      "Epoch 616, Loss: 42.929176330566406\n",
      "Epoch 617, Loss: 42.97992706298828\n",
      "Epoch 618, Loss: 42.99918746948242\n",
      "Epoch 619, Loss: 42.96664047241211\n",
      "Epoch 620, Loss: 42.98357391357422\n",
      "Epoch 621, Loss: 42.95461654663086\n",
      "Epoch 622, Loss: 42.91770935058594\n",
      "Epoch 623, Loss: 42.93790054321289\n",
      "Epoch 624, Loss: 42.902767181396484\n",
      "Epoch 625, Loss: 42.98011016845703\n",
      "Epoch 626, Loss: 42.916297912597656\n",
      "Epoch 627, Loss: 42.938053131103516\n",
      "Epoch 628, Loss: 42.929420471191406\n",
      "Epoch 629, Loss: 42.94335174560547\n",
      "Epoch 630, Loss: 42.9126091003418\n",
      "Epoch 631, Loss: 42.91823959350586\n",
      "Epoch 632, Loss: 42.960670471191406\n",
      "Epoch 633, Loss: 42.9517822265625\n",
      "Epoch 634, Loss: 42.95375061035156\n",
      "Epoch 635, Loss: 42.923370361328125\n",
      "Epoch 636, Loss: 42.99439239501953\n",
      "Epoch 637, Loss: 42.93478012084961\n",
      "Epoch 638, Loss: 42.92621612548828\n",
      "Epoch 639, Loss: 42.9289436340332\n",
      "Epoch 640, Loss: 42.89314651489258\n",
      "Epoch 641, Loss: 42.94705581665039\n",
      "Epoch 642, Loss: 42.92625427246094\n",
      "Epoch 643, Loss: 42.89064407348633\n",
      "Epoch 644, Loss: 42.89625549316406\n",
      "Epoch 645, Loss: 42.932212829589844\n",
      "Epoch 646, Loss: 42.912193298339844\n",
      "Epoch 647, Loss: 42.90449142456055\n",
      "Epoch 648, Loss: 42.91598129272461\n",
      "Epoch 649, Loss: 42.95123291015625\n",
      "Epoch 650, Loss: 42.98471450805664\n",
      "Epoch 651, Loss: 42.933013916015625\n",
      "Epoch 652, Loss: 42.93704605102539\n",
      "Epoch 653, Loss: 42.96099853515625\n",
      "Epoch 654, Loss: 42.932044982910156\n",
      "Epoch 655, Loss: 42.9548454284668\n",
      "Epoch 656, Loss: 42.92985534667969\n",
      "Epoch 657, Loss: 42.92811584472656\n",
      "Epoch 658, Loss: 42.904380798339844\n",
      "Epoch 659, Loss: 42.92445755004883\n",
      "Epoch 660, Loss: 42.91753387451172\n",
      "Epoch 661, Loss: 42.89849090576172\n",
      "Epoch 662, Loss: 42.93461227416992\n",
      "Epoch 663, Loss: 42.86686325073242\n",
      "Epoch 664, Loss: 42.91741180419922\n",
      "Epoch 665, Loss: 42.91768264770508\n",
      "Epoch 666, Loss: 42.966835021972656\n",
      "Epoch 667, Loss: 42.91582107543945\n",
      "Epoch 668, Loss: 42.955406188964844\n",
      "Epoch 669, Loss: 42.8544807434082\n",
      "Epoch 670, Loss: 42.945743560791016\n",
      "Epoch 671, Loss: 42.915409088134766\n",
      "Epoch 672, Loss: 42.87692642211914\n",
      "Epoch 673, Loss: 42.89250564575195\n",
      "Epoch 674, Loss: 42.888465881347656\n",
      "Epoch 675, Loss: 42.90601348876953\n",
      "Epoch 676, Loss: 42.89662551879883\n",
      "Epoch 677, Loss: 42.884613037109375\n",
      "Epoch 678, Loss: 42.86183166503906\n",
      "Epoch 679, Loss: 42.898765563964844\n",
      "Epoch 680, Loss: 42.90277862548828\n",
      "Epoch 681, Loss: 42.87967300415039\n",
      "Epoch 682, Loss: 42.871150970458984\n",
      "Epoch 683, Loss: 42.88376998901367\n",
      "Epoch 684, Loss: 42.911380767822266\n",
      "Epoch 685, Loss: 42.87653732299805\n",
      "Epoch 686, Loss: 42.88002014160156\n",
      "Epoch 687, Loss: 42.87979507446289\n",
      "Epoch 688, Loss: 42.86658477783203\n",
      "Epoch 689, Loss: 42.89439010620117\n",
      "Epoch 690, Loss: 42.885990142822266\n",
      "Epoch 691, Loss: 42.887882232666016\n",
      "Epoch 692, Loss: 42.87257766723633\n",
      "Epoch 693, Loss: 42.88157272338867\n",
      "Epoch 694, Loss: 42.906898498535156\n",
      "Epoch 695, Loss: 42.87825012207031\n",
      "Epoch 696, Loss: 42.877593994140625\n",
      "Epoch 697, Loss: 42.8978157043457\n",
      "Epoch 698, Loss: 42.94776916503906\n",
      "Epoch 699, Loss: 42.88083267211914\n",
      "Epoch 700, Loss: 42.85714340209961\n",
      "Epoch 701, Loss: 42.851680755615234\n",
      "Epoch 702, Loss: 42.889404296875\n",
      "Epoch 703, Loss: 42.83667755126953\n",
      "Epoch 704, Loss: 42.893821716308594\n",
      "Epoch 705, Loss: 42.88779067993164\n",
      "Epoch 706, Loss: 42.836544036865234\n",
      "Epoch 707, Loss: 42.875282287597656\n",
      "Epoch 708, Loss: 42.86473083496094\n",
      "Epoch 709, Loss: 42.88224411010742\n",
      "Epoch 710, Loss: 42.871620178222656\n",
      "Epoch 711, Loss: 42.86897277832031\n",
      "Epoch 712, Loss: 42.884521484375\n",
      "Epoch 713, Loss: 42.851837158203125\n",
      "Epoch 714, Loss: 42.88392639160156\n",
      "Epoch 715, Loss: 42.86806869506836\n",
      "Epoch 716, Loss: 42.84052276611328\n",
      "Epoch 717, Loss: 42.85042953491211\n",
      "Epoch 718, Loss: 42.84967803955078\n",
      "Epoch 719, Loss: 42.8535041809082\n",
      "Epoch 720, Loss: 42.879817962646484\n",
      "Epoch 721, Loss: 42.869136810302734\n",
      "Epoch 722, Loss: 42.88641357421875\n",
      "Epoch 723, Loss: 42.83832931518555\n",
      "Epoch 724, Loss: 42.8685188293457\n",
      "Epoch 725, Loss: 42.85538101196289\n",
      "Epoch 726, Loss: 42.85823059082031\n",
      "Epoch 727, Loss: 42.8582878112793\n",
      "Epoch 728, Loss: 42.84957504272461\n",
      "Epoch 729, Loss: 42.87751388549805\n",
      "Epoch 730, Loss: 42.8281364440918\n",
      "Epoch 731, Loss: 42.84648895263672\n",
      "Epoch 732, Loss: 42.84379196166992\n",
      "Epoch 733, Loss: 42.847904205322266\n",
      "Epoch 734, Loss: 42.84975051879883\n",
      "Epoch 735, Loss: 42.87060546875\n",
      "Epoch 736, Loss: 42.881439208984375\n",
      "Epoch 737, Loss: 42.87400817871094\n",
      "Epoch 738, Loss: 42.83468246459961\n",
      "Epoch 739, Loss: 42.8711051940918\n",
      "Epoch 740, Loss: 42.862831115722656\n",
      "Epoch 741, Loss: 42.8587760925293\n",
      "Epoch 742, Loss: 42.880897521972656\n",
      "Epoch 743, Loss: 42.86183166503906\n",
      "Epoch 744, Loss: 42.8908576965332\n",
      "Epoch 745, Loss: 42.8484001159668\n",
      "Epoch 746, Loss: 42.87078857421875\n",
      "Epoch 747, Loss: 42.841007232666016\n",
      "Epoch 748, Loss: 42.83839416503906\n",
      "Epoch 749, Loss: 42.864986419677734\n",
      "Epoch 750, Loss: 42.84613800048828\n",
      "Epoch 751, Loss: 42.842891693115234\n",
      "Epoch 752, Loss: 42.856544494628906\n",
      "Epoch 753, Loss: 42.83683776855469\n",
      "Epoch 754, Loss: 42.84183883666992\n",
      "Epoch 755, Loss: 42.830810546875\n",
      "Epoch 756, Loss: 42.81598663330078\n",
      "Epoch 757, Loss: 42.835060119628906\n",
      "Epoch 758, Loss: 42.843265533447266\n",
      "Epoch 759, Loss: 42.832252502441406\n",
      "Epoch 760, Loss: 42.842227935791016\n",
      "Epoch 761, Loss: 42.822872161865234\n",
      "Epoch 762, Loss: 42.84101104736328\n",
      "Epoch 763, Loss: 42.8248405456543\n",
      "Epoch 764, Loss: 42.84806823730469\n",
      "Epoch 765, Loss: 42.81589889526367\n",
      "Epoch 766, Loss: 42.79568862915039\n",
      "Epoch 767, Loss: 42.812744140625\n",
      "Epoch 768, Loss: 42.861961364746094\n",
      "Epoch 769, Loss: 42.844261169433594\n",
      "Epoch 770, Loss: 42.82596206665039\n",
      "Epoch 771, Loss: 42.80924987792969\n",
      "Epoch 772, Loss: 42.8446044921875\n",
      "Epoch 773, Loss: 42.820213317871094\n",
      "Epoch 774, Loss: 42.80363845825195\n",
      "Epoch 775, Loss: 42.8044548034668\n",
      "Epoch 776, Loss: 42.84078598022461\n",
      "Epoch 777, Loss: 42.83226776123047\n",
      "Epoch 778, Loss: 42.81891632080078\n",
      "Epoch 779, Loss: 42.831668853759766\n",
      "Epoch 780, Loss: 42.850677490234375\n",
      "Epoch 781, Loss: 42.816707611083984\n",
      "Epoch 782, Loss: 42.820377349853516\n",
      "Epoch 783, Loss: 42.8372688293457\n",
      "Epoch 784, Loss: 42.833648681640625\n",
      "Epoch 785, Loss: 42.84199142456055\n",
      "Epoch 786, Loss: 42.81741714477539\n",
      "Epoch 787, Loss: 42.839927673339844\n",
      "Epoch 788, Loss: 42.8355827331543\n",
      "Epoch 789, Loss: 42.82100296020508\n",
      "Epoch 790, Loss: 42.815547943115234\n",
      "Epoch 791, Loss: 42.83018493652344\n",
      "Epoch 792, Loss: 42.822486877441406\n",
      "Epoch 793, Loss: 42.84617233276367\n",
      "Epoch 794, Loss: 42.78605270385742\n",
      "Epoch 795, Loss: 42.81437301635742\n",
      "Epoch 796, Loss: 42.823829650878906\n",
      "Epoch 797, Loss: 42.81928634643555\n",
      "Epoch 798, Loss: 42.8132209777832\n",
      "Epoch 799, Loss: 42.82191467285156\n",
      "Epoch 800, Loss: 42.82966232299805\n",
      "Epoch 801, Loss: 42.83086013793945\n",
      "Epoch 802, Loss: 42.81603240966797\n",
      "Epoch 803, Loss: 42.791175842285156\n",
      "Epoch 804, Loss: 42.81920623779297\n",
      "Epoch 805, Loss: 42.83784866333008\n",
      "Epoch 806, Loss: 42.82062530517578\n",
      "Epoch 807, Loss: 42.8095588684082\n",
      "Epoch 808, Loss: 42.817054748535156\n",
      "Epoch 809, Loss: 42.79396438598633\n",
      "Epoch 810, Loss: 42.80185317993164\n",
      "Epoch 811, Loss: 42.79282760620117\n",
      "Epoch 812, Loss: 42.792320251464844\n",
      "Epoch 813, Loss: 42.804134368896484\n",
      "Epoch 814, Loss: 42.803707122802734\n",
      "Epoch 815, Loss: 42.8061408996582\n",
      "Epoch 816, Loss: 42.81850051879883\n",
      "Epoch 817, Loss: 42.78807067871094\n",
      "Epoch 818, Loss: 42.77125549316406\n",
      "Epoch 819, Loss: 42.81672668457031\n",
      "Epoch 820, Loss: 42.81481170654297\n",
      "Epoch 821, Loss: 42.80994415283203\n",
      "Epoch 822, Loss: 42.804935455322266\n",
      "Epoch 823, Loss: 42.7989616394043\n",
      "Epoch 824, Loss: 42.82050704956055\n",
      "Epoch 825, Loss: 42.83617401123047\n",
      "Epoch 826, Loss: 42.81431198120117\n",
      "Epoch 827, Loss: 42.788150787353516\n",
      "Epoch 828, Loss: 42.819576263427734\n",
      "Epoch 829, Loss: 42.80155563354492\n",
      "Epoch 830, Loss: 42.81690216064453\n",
      "Epoch 831, Loss: 42.77442932128906\n",
      "Epoch 832, Loss: 42.803443908691406\n",
      "Epoch 833, Loss: 42.79962921142578\n",
      "Epoch 834, Loss: 42.83059310913086\n",
      "Epoch 835, Loss: 42.808197021484375\n",
      "Epoch 836, Loss: 42.790035247802734\n",
      "Epoch 837, Loss: 42.80977249145508\n",
      "Epoch 838, Loss: 42.8048210144043\n",
      "Epoch 839, Loss: 42.811405181884766\n",
      "Epoch 840, Loss: 42.79787063598633\n",
      "Epoch 841, Loss: 42.796051025390625\n",
      "Epoch 842, Loss: 42.781005859375\n",
      "Epoch 843, Loss: 42.79836654663086\n",
      "Epoch 844, Loss: 42.78913879394531\n",
      "Epoch 845, Loss: 42.80339431762695\n",
      "Epoch 846, Loss: 42.779090881347656\n",
      "Epoch 847, Loss: 42.7847785949707\n",
      "Epoch 848, Loss: 42.78640365600586\n",
      "Epoch 849, Loss: 42.79563903808594\n",
      "Epoch 850, Loss: 42.785377502441406\n",
      "Epoch 851, Loss: 42.79958724975586\n",
      "Epoch 852, Loss: 42.78841018676758\n",
      "Epoch 853, Loss: 42.79597473144531\n",
      "Epoch 854, Loss: 42.760902404785156\n",
      "Epoch 855, Loss: 42.800228118896484\n",
      "Epoch 856, Loss: 42.786075592041016\n",
      "Epoch 857, Loss: 42.787506103515625\n",
      "Epoch 858, Loss: 42.79832458496094\n",
      "Epoch 859, Loss: 42.76687240600586\n",
      "Epoch 860, Loss: 42.79352569580078\n",
      "Epoch 861, Loss: 42.78081512451172\n",
      "Epoch 862, Loss: 42.785709381103516\n",
      "Epoch 863, Loss: 42.784786224365234\n",
      "Epoch 864, Loss: 42.781707763671875\n",
      "Epoch 865, Loss: 42.800472259521484\n",
      "Epoch 866, Loss: 42.781150817871094\n",
      "Epoch 867, Loss: 42.77942657470703\n",
      "Epoch 868, Loss: 42.76116180419922\n",
      "Epoch 869, Loss: 42.79555892944336\n",
      "Epoch 870, Loss: 42.77623748779297\n",
      "Epoch 871, Loss: 42.788673400878906\n",
      "Epoch 872, Loss: 42.75626754760742\n",
      "Epoch 873, Loss: 42.767425537109375\n",
      "Epoch 874, Loss: 42.78458023071289\n",
      "Epoch 875, Loss: 42.76547622680664\n",
      "Epoch 876, Loss: 42.776798248291016\n",
      "Epoch 877, Loss: 42.77928924560547\n",
      "Epoch 878, Loss: 42.76639175415039\n",
      "Epoch 879, Loss: 42.75186538696289\n",
      "Epoch 880, Loss: 42.77763366699219\n",
      "Epoch 881, Loss: 42.77375411987305\n",
      "Epoch 882, Loss: 42.77714157104492\n",
      "Epoch 883, Loss: 42.790714263916016\n",
      "Epoch 884, Loss: 42.76925277709961\n",
      "Epoch 885, Loss: 42.78649139404297\n",
      "Epoch 886, Loss: 42.75651550292969\n",
      "Epoch 887, Loss: 42.77060317993164\n",
      "Epoch 888, Loss: 42.781978607177734\n",
      "Epoch 889, Loss: 42.766929626464844\n",
      "Epoch 890, Loss: 42.77151870727539\n",
      "Epoch 891, Loss: 42.75819396972656\n",
      "Epoch 892, Loss: 42.758358001708984\n",
      "Epoch 893, Loss: 42.76266860961914\n",
      "Epoch 894, Loss: 42.77133560180664\n",
      "Epoch 895, Loss: 42.77264404296875\n",
      "Epoch 896, Loss: 42.768741607666016\n",
      "Epoch 897, Loss: 42.7757682800293\n",
      "Epoch 898, Loss: 42.752323150634766\n",
      "Epoch 899, Loss: 42.762203216552734\n",
      "Epoch 900, Loss: 42.74543762207031\n",
      "Epoch 901, Loss: 42.78529357910156\n",
      "Epoch 902, Loss: 42.78938293457031\n",
      "Epoch 903, Loss: 42.7800407409668\n",
      "Epoch 904, Loss: 42.76291275024414\n",
      "Epoch 905, Loss: 42.77001190185547\n",
      "Epoch 906, Loss: 42.77983856201172\n",
      "Epoch 907, Loss: 42.77976989746094\n",
      "Epoch 908, Loss: 42.771907806396484\n",
      "Epoch 909, Loss: 42.7562141418457\n",
      "Epoch 910, Loss: 42.76105499267578\n",
      "Epoch 911, Loss: 42.75858688354492\n",
      "Epoch 912, Loss: 42.76251983642578\n",
      "Epoch 913, Loss: 42.77097702026367\n",
      "Epoch 914, Loss: 42.7715950012207\n",
      "Epoch 915, Loss: 42.785301208496094\n",
      "Epoch 916, Loss: 42.766300201416016\n",
      "Epoch 917, Loss: 42.772064208984375\n",
      "Epoch 918, Loss: 42.771602630615234\n",
      "Epoch 919, Loss: 42.76011657714844\n",
      "Epoch 920, Loss: 42.755409240722656\n",
      "Epoch 921, Loss: 42.77889633178711\n",
      "Epoch 922, Loss: 42.75215530395508\n",
      "Epoch 923, Loss: 42.75851058959961\n",
      "Epoch 924, Loss: 42.74166488647461\n",
      "Epoch 925, Loss: 42.741432189941406\n",
      "Epoch 926, Loss: 42.76673126220703\n",
      "Epoch 927, Loss: 42.7644157409668\n",
      "Epoch 928, Loss: 42.750606536865234\n",
      "Epoch 929, Loss: 42.756202697753906\n",
      "Epoch 930, Loss: 42.76094436645508\n",
      "Epoch 931, Loss: 42.751686096191406\n",
      "Epoch 932, Loss: 42.7401237487793\n",
      "Epoch 933, Loss: 42.76433563232422\n",
      "Epoch 934, Loss: 42.75803756713867\n",
      "Epoch 935, Loss: 42.75567626953125\n",
      "Epoch 936, Loss: 42.74705505371094\n",
      "Epoch 937, Loss: 42.74405288696289\n",
      "Epoch 938, Loss: 42.7772331237793\n",
      "Epoch 939, Loss: 42.77210998535156\n",
      "Epoch 940, Loss: 42.76188278198242\n",
      "Epoch 941, Loss: 42.7579460144043\n",
      "Epoch 942, Loss: 42.75506591796875\n",
      "Epoch 943, Loss: 42.747371673583984\n",
      "Epoch 944, Loss: 42.763389587402344\n",
      "Epoch 945, Loss: 42.73887252807617\n",
      "Epoch 946, Loss: 42.731502532958984\n",
      "Epoch 947, Loss: 42.764095306396484\n",
      "Epoch 948, Loss: 42.73423767089844\n",
      "Epoch 949, Loss: 42.756683349609375\n",
      "Epoch 950, Loss: 42.72947692871094\n",
      "Epoch 951, Loss: 42.74843978881836\n",
      "Epoch 952, Loss: 42.72462844848633\n",
      "Epoch 953, Loss: 42.75304412841797\n",
      "Epoch 954, Loss: 42.77061462402344\n",
      "Epoch 955, Loss: 42.74690246582031\n",
      "Epoch 956, Loss: 42.75960922241211\n",
      "Epoch 957, Loss: 42.74962615966797\n",
      "Epoch 958, Loss: 42.74798583984375\n",
      "Epoch 959, Loss: 42.779354095458984\n",
      "Epoch 960, Loss: 42.744869232177734\n",
      "Epoch 961, Loss: 42.75023651123047\n",
      "Epoch 962, Loss: 42.74369430541992\n",
      "Epoch 963, Loss: 42.74616622924805\n",
      "Epoch 964, Loss: 42.75991439819336\n",
      "Epoch 965, Loss: 42.75831604003906\n",
      "Epoch 966, Loss: 42.7421875\n",
      "Epoch 967, Loss: 42.770503997802734\n",
      "Epoch 968, Loss: 42.74840545654297\n",
      "Epoch 969, Loss: 42.74199676513672\n",
      "Epoch 970, Loss: 42.730377197265625\n",
      "Epoch 971, Loss: 42.753231048583984\n",
      "Epoch 972, Loss: 42.722572326660156\n",
      "Epoch 973, Loss: 42.73980712890625\n",
      "Epoch 974, Loss: 42.729888916015625\n",
      "Epoch 975, Loss: 42.74347686767578\n",
      "Epoch 976, Loss: 42.731807708740234\n",
      "Epoch 977, Loss: 42.753665924072266\n",
      "Epoch 978, Loss: 42.74903869628906\n",
      "Epoch 979, Loss: 42.742855072021484\n",
      "Epoch 980, Loss: 42.760398864746094\n",
      "Epoch 981, Loss: 42.72653579711914\n",
      "Epoch 982, Loss: 42.74556350708008\n",
      "Epoch 983, Loss: 42.73624801635742\n",
      "Epoch 984, Loss: 42.732574462890625\n",
      "Epoch 985, Loss: 42.73494338989258\n",
      "Epoch 986, Loss: 42.74690628051758\n",
      "Epoch 987, Loss: 42.73986053466797\n",
      "Epoch 988, Loss: 42.725425720214844\n",
      "Epoch 989, Loss: 42.74536895751953\n",
      "Epoch 990, Loss: 42.727874755859375\n",
      "Epoch 991, Loss: 42.74482345581055\n",
      "Epoch 992, Loss: 42.72969436645508\n",
      "Epoch 993, Loss: 42.73009490966797\n",
      "Epoch 994, Loss: 42.742156982421875\n",
      "Epoch 995, Loss: 42.723148345947266\n",
      "Epoch 996, Loss: 42.72200393676758\n",
      "Epoch 997, Loss: 42.724952697753906\n",
      "Epoch 998, Loss: 42.751522064208984\n",
      "Epoch 999, Loss: 42.7328987121582\n",
      "Epoch 1000, Loss: 42.75753402709961\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 50)\n",
    "        self.fc21 = nn.Linear(50, 2)\n",
    "        self.fc22 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc3 = nn.Linear(2, 50)\n",
    "        self.fc4 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "# Define the VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x.view(-1, 2))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 2), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    beta = 5e-5\n",
    "    return BCE + beta*KLD\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Dummy dataset\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(1000):  # Number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    recon_batch, mu, logvar = model(data)\n",
    "    loss = loss_function(recon_batch, data, mu, logvar)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Plotting the latent space\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, mu, _ = model(data)\n",
    "    plt, ax = plt.subplots()\n",
    "    ax.scatter(mu[:, 0].cpu(), mu[:, 1].cpu(), c=clrs, marker='o')\n",
    "    ax.set_title('Latent Space')\n",
    "    ax.set_xlabel('Latent Dimension 1')\n",
    "    ax.set_ylabel('Latent Dimension 2')\n",
    "    for i,j in enumerate(txt):\n",
    "        ax.annotate(j, (mu[i, 0].cpu().numpy(), mu[i, 1].cpu().numpy()))\n",
    "\n",
    "\n",
    "for i in [0,1]:\n",
    "    pts = np.array([mu[25*i:25*(i+1),0].cpu().numpy(), mu[25*i:25*(i+1),1].cpu().numpy()]).T\n",
    "    enclosing_ellipse = welzl(np.array(pts, dtype=float))\n",
    "    # plot resulting ellipse\n",
    "    center,a,b,t = enclosing_ellipse\n",
    "    elli = plot_ellipse(enclosing_ellipse, str='k')\n",
    "    ellipse = Ellipse(xy=center, width=2*a, height=2*b, angle=np.degrees(t), edgecolor='k', fc='r' if i == 0 else 'b', alpha=0.3, lw=2)\n",
    "    ax.add_patch(ellipse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Figure' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Figure' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
